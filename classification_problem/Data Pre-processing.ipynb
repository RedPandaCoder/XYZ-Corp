{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfc7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/XYZCorp_LendingData.txt',sep='\\t', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2467e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(file = None):\n",
    "    import pandas as pd\n",
    "    \n",
    "    if file is None: \n",
    "        raise Exception('a file path or pandas dataframe must be given')\n",
    "        \n",
    "    if type(file)==str:\n",
    "        try:\n",
    "            dataset = pd.read_csv(file,sep='\\t', low_memory = False)\n",
    "        except: \n",
    "            raise Exception('file path {} not found'.format(file))\n",
    "            \n",
    "    elif type(file)!=pd.core.frame.DataFrame:\n",
    "        raise Exception('a file path or pandas dataframe must be given'.format(file))\n",
    "        \n",
    "    if type(file)==pd.core.frame.DataFrame:\n",
    "        dataset = file\n",
    "            \n",
    "    categorical_elements = ['term',\n",
    "                            'initial_list_status',\n",
    "                            'application_type',\n",
    "                            'grade',\n",
    "                            'home_ownership',\n",
    "                            'verification_status',\n",
    "                            'purpose']\n",
    "    \n",
    "    categorical_features = ['term_ 60 months', 'initial_list_status_w', 'application_type_JOINT',\n",
    "                            'grade_B', 'grade_C', 'grade_D', 'grade_E', 'grade_F', 'grade_G',\n",
    "                            'home_ownership_MORTGAGE', 'home_ownership_NONE',\n",
    "                            'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT',\n",
    "                            'verification_status_Source Verified', 'verification_status_Verified',\n",
    "                            'purpose_credit_card', 'purpose_debt_consolidation',\n",
    "                            'purpose_educational', 'purpose_home_improvement', 'purpose_house',\n",
    "                            'purpose_major_purchase', 'purpose_medical', 'purpose_moving',\n",
    "                            'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',\n",
    "                            'purpose_vacation', 'purpose_wedding']\n",
    "    \n",
    "    date_columns=['last_pymnt_d',\n",
    "                  'last_credit_pull_d',\n",
    "                  'issue_d',\n",
    "                  'earliest_cr_line']\n",
    "    \n",
    "    num_features = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate',\n",
    "                    'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths',\n",
    "                    'mths_since_last_delinq', 'open_acc', 'pub_rec', 'revol_bal',\n",
    "                    'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
    "                    'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'last_pymnt_amnt', 'tot_coll_amt',\n",
    "                    'tot_cur_bal', 'total_rev_hi_lim']\n",
    "    \n",
    "    derived_features = ['months_since_last_payment','months_since_last_credit_pull',\n",
    "                        'months_since_earliest_cr_line',\n",
    "                        'emp_length','late_fees_applied']\n",
    "\n",
    "    # Clipping the values to 95% Quantile\n",
    "    for column in num_features:\n",
    "        upp = dataset[column].quantile(0.975)\n",
    "        low = dataset[column].quantile(0.025)\n",
    "        dataset[column] = dataset[column].clip(upper = upp, lower = low)\n",
    "    \n",
    "    categorical_dummies =  pd.get_dummies(dataset[categorical_elements], drop_first=True)\n",
    "    \n",
    "    for column in categorical_features:\n",
    "        if column not in categorical_dummies.columns:\n",
    "            categorical_dummies[column] = 0\n",
    "            categorical_dummies[column] = categorical_dummies[column].astype('uint8')\n",
    "\n",
    "    dataset = pd.concat([dataset, categorical_dummies], axis = 1)\n",
    "    \n",
    "     #Converting Date Columns to Number of Months since\n",
    "    for column in date_columns:\n",
    "            dataset[column]=pd.to_datetime(dataset[column])\n",
    "\n",
    "    dataset['current_date'] = pd.to_datetime(\"2017-01-01\")\n",
    "\n",
    "    dataset['months_since_last_payment'] = ((dataset.current_date.dt.year  - dataset.last_pymnt_d.dt.year) * 12 +\n",
    "                                         (dataset.current_date.dt.month - dataset.last_pymnt_d.dt.month))\n",
    "    dataset['months_since_last_credit_pull'] = ((dataset.current_date.dt.year  - dataset.last_credit_pull_d.dt.year) * 12 +\n",
    "                                         (dataset.current_date.dt.month - dataset.last_pymnt_d.dt.month))\n",
    "    # dataset['months_since_issue'] = ((dataset.current_date.dt.year  - dataset.issue_d.dt.year) * 12 +\n",
    "    #                                      (dataset.current_date.dt.month - dataset.last_pymnt_d.dt.month))\n",
    "    dataset['months_since_earliest_cr_line'] = ((dataset.current_date.dt.year  - dataset.earliest_cr_line.dt.year) * 12 +\n",
    "                                         (dataset.current_date.dt.month - dataset.last_pymnt_d.dt.month))\n",
    "    emp_dict = {'< 1 year':0, '1 year':1, '2 years':2,\n",
    "                '3 years':3, '4 years':4, '5 years':5,\n",
    "                '6 years':6, '7 years':7, '8 years':8,\n",
    "                '9 years':9, '10+ years':10}\n",
    "    dataset['emp_length'] = dataset['emp_length'].map(emp_dict)\n",
    "    \n",
    "    # derived feature for to check if late fees were applied\n",
    "    dataset['late_fees_applied']= np.where(dataset['total_rec_late_fee']>0, 1, 0)\n",
    "    \n",
    "    # filling in nulls - 0 for employment length, median for all others numerical and derived features.\n",
    "    dataset['emp_length'] = dataset['emp_length'].fillna(0)\n",
    "    \n",
    "    for column in num_features + derived_features:\n",
    "        dataset[column]=dataset[column].fillna(dataset[column].median())\n",
    "    \n",
    "    all_features = num_features+categorical_features+derived_features+['issue_d']\n",
    "    \n",
    "    return dataset[all_features+['default_ind']], all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06a521cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(dataset, all_features, random_split = False ,undersample = False, oversample = False):\n",
    "\n",
    "    if undersample & oversample:\n",
    "        return print('please pick only one - over or under sample method')\n",
    "    \n",
    "    if random_split == True:\n",
    "        X=dataset.drop('issue_d',axis=1)\n",
    "        y=dataset[['default_ind']]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, stratify = y)\n",
    "    else:\n",
    "        X=dataset[all_features]\n",
    "        y=dataset[['default_ind','issue_d']]\n",
    "\n",
    "        X_train=X[X['issue_d']<='2015-05-31'].drop('issue_d',axis=1)\n",
    "        X_test=X[X['issue_d']>'2015-05-31'].drop('issue_d',axis=1)\n",
    "\n",
    "        y_train=y[y['issue_d']<='2015-05-31'].drop('issue_d',axis=1)\n",
    "        y_test=y[y['issue_d']>'2015-05-31'].drop('issue_d',axis=1)\n",
    "    \n",
    "    if undersample == True:\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train,y_train = undersampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "    if oversample == True:\n",
    "        oversampler = SMOTE()\n",
    "        undersampler = RandomUnderSampler(sampling_strategy=0.461556)\n",
    "        X_train,y_train = undersampler.fit_resample(X_train, y_train)\n",
    "        X_train,y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train, X_test, np.ravel(y_train), np.ravel(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, features = pre_processing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be033f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_split(data, features, random_split = False, oversample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c07c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default_ind\n",
       "0    100000\n",
       "1    100000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.groupby('default_ind').size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
